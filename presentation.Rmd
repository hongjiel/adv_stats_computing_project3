---
title: |
  | Bayesian Modeling of Hurricane Trajectories
author: |
  | Hongjie Liu, Xicheng Xie, Jiajun Tao, Zijian Xu, Shaohan Chen
date: "May 1st, 2023"
header-includes:
   - \usepackage{bm}
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
   - \usepackage{algorithm}
   - \usepackage{algpseudocode}
output:
  beamer_presentation:
    colortheme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(sigmoid) 
library(qgam) 
library(pROC)
library(xtable)
library(kableExtra)
library(boot) 
library(ggplot2)
library(gridExtra)
# magic that automatically adjusts the font size
def.chunk.hook = knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x = def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```


## Outline

- Background

- Task 1

- Task 2

- Task 3

- Task 4

- Discussions

- Reference

- Q&A


## Background
**Bayesian Modeling of Hurricane Trajectories:**\par
- In this project we are interested in modeling the hurricane trajectories to forecast the wind speed.

## Background 
**Data Source:**\par
- "hurricane703.csv" collected the track data of 703 hurricanes in the North Atlantic area since 1950. For all the storms, their location (longitude & latitude) and maximum wind speed were recorded every 6 hours.


## Task 1
**Objective:**

Let $B = (\beta_1^T, ..., \beta_n^T)$, derive the posterior distribution of the parameters $\Theta = (\mathbf{B}^T, \bm{\mu}^T, \sigma^2, \Sigma)$


## Task 2
**Objective:**\par

Design and implement a custom MCMC algorithm for the outlined Bayesian hierarchical model. Monitor the convergence of the MCMC chains, using diagnostic plots and summary statistics to check for any issues.

## Task 2 - MCMC Algorithm
\begin{algorithm}[H]
  \fontsize{5pt}{4pt}\selectfont
 \caption{MCMC Algorithm (Part 1)}
 \begin{algorithmic}
   \Require $\mathbf{Y}$; $\boldsymbol{\beta}_0, \boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0, \sigma_0, \gamma_0$
   \Ensure $\widehat{\boldsymbol{\beta}}, \widehat{\boldsymbol{\mu}}, \widehat{\boldsymbol{\Sigma}}, \widehat{\sigma}, \widehat{\gamma}$ $\approx$ $\boldsymbol{\beta}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \sigma, \gamma$
   \State $i\leftarrow 0$, where $i$ is the current number of iterations
   \While {iteration times is not met}
     \State $i \leftarrow i+1$
     \State Gibbs sampling for $\boldsymbol{\beta}$\par ${\boldsymbol{\beta}^{(k)}} \sim \mathcal{N}(\boldsymbol{\beta}|\mathbf{Y},\boldsymbol{\mu}^{(k-1)},\boldsymbol{\Sigma}^{(k-1)}, \sigma^{(k-1)}, \boldsymbol{\gamma}^{(k-1)})$
     \State Gibbs sampling for $\boldsymbol{\mu}$\par $\boldsymbol{\mu}^{(k)} \sim \mathcal{N}(\boldsymbol{\mu}|\mathbf{Y}, \boldsymbol{\beta}^{(k)}, \boldsymbol{\Sigma}^{(k-1)}, \sigma^{(k-1)}, \boldsymbol{\gamma}^{(k-1)})$
     \State Gibbs sampling for $\Sigma$\par $\boldsymbol{\Sigma}^{(k)} \sim \mathcal{Wishart}(\boldsymbol{\Sigma}|\mathbf{Y}, \boldsymbol{\beta}^{(k)}, \boldsymbol{\mu}^{(k)}, \sigma^{(k-1)}, \gamma^{(k-1)})$
     \State Gibbs sampling for $\boldsymbol{\gamma}$ $\boldsymbol{\gamma}^{(k)} \sim p(\boldsymbol{\Sigma}|\mathbf{Y}, \boldsymbol{\beta}^{(k)}, \boldsymbol{\mu}^{(k)}, , \Sigma^{(k)},\sigma^{(k-1)})$
     \State Metropolis-Hastings for $\sigma$
      \State Propose a new value $\sigma^*$ from a normal distribution with mean $\sigma^{(k-1)}$ and a small variance.
      \State Compute the acceptance ratio\par
      $\gamma = \frac{p(\gamma^{(k)}|\mathbf{Y}, \boldsymbol{\beta}^{(k)}, \boldsymbol{\mu}^{(k)}, \boldsymbol{\Sigma}^{(k)},\sigma^{(k)})q(\gamma^{(k-1)}|\gamma^{(k)})}{p(\gamma^{(k-1)}|\mathbf{Y}, \boldsymbol{\beta}^{(k)}, \boldsymbol{\mu}^{(k)}, \boldsymbol{\Sigma}^{(k)}, \sigma^{(k)})q(\gamma^{(k)}|\gamma^{(k-1)})}$
      \State Generate a random number $u$ from a uniform distribution between 0 and 1.
      \State If $u \leq r$, set $\sigma^{(k)} = \sigma^*$, otherwise set $\sigma^{(k)} = \sigma^{(k-1)}$.
      \State 
    \EndWhile
  \State 
  \end{algorithmic}
\end{algorithm}


## Task 2 - Starting Values
\tiny
 \begin{columns}
  \begin{column}{0.4\textwidth}
    \textbf{Final Initial Value Selection and Core Information of MH Algorithm}
    \begin{itemize}
    \item$\boldsymbol{\beta}_i$: This can be obtained through the random effects term in the lmm model. The random effects term can be added to the fixed effects term to obtain $\boldsymbol{\beta}_i^{(0)}$.
    \item$\boldsymbol{\mu}$: This can be obtained through the fixed effects of windpre, latdiff, longdiff, winddiff term.
    \item$\boldsymbol{\gamma}$: This can be obtained through the fixed effects of Season, Active Month, and Nature term.
    \item$\sigma^2$: This can be obtained through the model residual sigma0.
    \item$\Sigma^{-1}$: This can be obtained through the VarCorr(lmm) function which returns the covariance matrix of the random effects in the model. The inverse of this matrix can be taken to obtain $\Sigma^{-1^{(0)}}$.
    \end{itemize}
  \end{column}
 \begin{column}{0.6\textwidth}
 \centering
   \begin{table}[h]
   \centering
   \caption{Initial Value Setting}
   \begin{tabular}{|c|c|}
   \hline
   Parameter & Value \\
   \hline
   $\boldsymbol{\mu}$ & $(24.25, 0.94, -0.02, -0.24, 0.47)$ \\
   $\boldsymbol{\gamma}$ & $(-0.01, 0.35, 0.28, 0.37, 0.12, 0.08)$ \\
   $\Sigma$ & $\begin{pmatrix}
   0.36 & -0.01 & 0.04 & 0.12 & 0.03 \\
   -0.01 & 0.00 & -0.00 & -0.00 & 0.00 \\
   0.04 & -0.00 & 0.04 & 0.03 & -0.02 \\
   0.12 & -0.00 & 0.03 & 0.07 & 0.00 \\
   0.03 & 0.00 & -0.02 & 0.00 & 0.02 \\
    \end{pmatrix}$ \\
   $\sigma^2$ & 5.27 \\
   \hline
   \end{tabular}
   \end{table}
  \end{column}
 \end{columns}
 
## Task 2 - MCMC Algorithm R code
```{r eval=FALSE, size= "tiny"}
# Gimbble sampling algorithm
B_sample <- function(mu, Sigma, gamma, sigma) {
  Sigma.inv <- solve(Sigma)
  B_mean_cov <- function(i) {
    cov <- solve(Sigma.inv + 1/sigma^2 * t(Z[[i]]) %*% Z[[i]])
    mean <- cov %*% (Sigma.inv %*% mu + 1/sigma^2 * colSums((Y[[i]] - (X[i,] %*% gamma)[,]) * Z[[i]]))
    list(mean = mean, cov = cov)
  }
  mean_cov_list <- lapply(1:n, B_mean_cov)
  B <- sapply(mean_cov_list, function(x) {mvrnorm(mu = x$mean, Sigma = x$cov)})
  return(B)
}

# MH algorithm (random walk)
sigma_sample <- function(sigma, B, gamma, a) {
  sigma_new <- sigma + (runif(1) - 0.5) * 2 * a # candidate sigma
  if (sigma_new <= 0) {
    return(sigma)
  }
  RSS <- sum(sapply(1:n, function(i) sum((Y[[i]] - Z[[i]] %*% B[,i] - (X[i,] %*% gamma)[,])^2)))
  log_kernal_ratio <- -sum(m) * log(sigma_new/sigma) +
    log(1 + (sigma/10)^2) - log(1 + (sigma_new/10)^2) -
    0.5 * (1/sigma_new^2 - 1/sigma^2) * RSS
  log_prob <- min(0, log_kernal_ratio)
  sigma <- ifelse(log_prob > log(runif(1)), sigma_new, sigma)
  return(sigma)
}
```


## Task 2 - Results Presentation(Time Series Plot for Each Parameters(Burn-In 8000))
\begin{figure} 
  \centering 
  \subfigure[Mu]{ 
    \label{sub1}
    \includegraphics[width=2.0in, height = 1.4in]{parameters_burnin/Mu_parameters_time_series_plot.png} 
  } 
  \subfigure[Sigma]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{parameters_burnin/Sigma_parameters_time_series_plot.png} 
  } 
  \subfigure[Gamma]{ 
    \label{sub3} 
    \includegraphics[width=2.0in, height = 1.4in]{parameters_burnin/Gamma_parameters_time_series_plot.png} 
  } 
  \subfigure[sigma]{ 
    \label{sub4} 
    \includegraphics[width=2.0in, height = 1.4in]{parameters_burnin/igma_parameters_time_series_plot.png} 
  } 
  \label{para1} 
\end{figure}

## Task 2 - Results Presentation(Histogram Plot for Mu)
```{r warning=FALSE, echo=FALSE}
mu_list <- read.csv("./data/mu_list.csv")
# Get the last 2000 rows of the mu_list data
mu_data <- tail(mu_list, 2000)

# Convert the mu_data to a data frame
mu_df <- as.data.frame(mu_data)

# Set the number of bins for the histogram
num_bins <- 30

# Define a color palette for the histograms
colors <- c("red", "green", "blue", "orange", "purple")

# Plot the histograms for each column in mu_data using ggplot
histograms <- lapply(seq_along(mu_df), function(i) {
  ggplot(mu_df, aes(x = mu_df[, i])) +
    geom_histogram(bins = num_bins, color = "black", fill = colors[i], alpha = 0.7) +
    ggtitle(paste0("mu_", i - 1)) +
    xlab("Value")
})

# Combine the histograms into a single plot using gridExtra
grid.arrange(grobs = histograms, ncol = 3)
```

##  Task 2 - Results Presentation(Values of $mu$ and $Sigma$)
\centering
\tiny
\textbf{$\hat{\mu}$}
\[
\begin{pmatrix}
  \hat{\mu}_0 & 4.6170057 \\
  \hat{\mu}_1 & 0.9035517 \\
  \hat{\mu}_2 & -0.0425631 \\
  \hat{\mu}_3 & -0.4558512 \\
  \hat{\mu}_4 & 0.4706897
\end{pmatrix}
\]

\centering
\tiny
\textbf{$\hat{\Sigma}$}
\[
\begin{pmatrix}
  & 0.7521 & -0.0155 & -0.0860 & 0.0136 & -0.0060\\
  & -0.0155 & 0.0050 & -0.0023 & -0.0013 & 0.0006\\
  & -0.0860 & -0.0023 & 0.2705 & -0.0085 & -0.0024\\
  & 0.0136 & -0.0013 & -0.0085 & 0.1287 & 0.0057\\
  & -0.0060 & 0.0006 & -0.0024 & 0.0057 & 0.0268
\end{pmatrix}
\]

\centering
\tiny
\textbf{$\hat{\rho}$}
\[
\begin{pmatrix}
  & 1.0000 & -0.2529 & -0.1907 & 0.0436 & -0.0423\\
  & -0.2529 & 1.0000 & -0.0635 & -0.0521 & 0.0490\\
  & -0.1907 & -0.0635 & 1.0000 & -0.0457 & -0.0277\\
  & 0.0436 & -0.0521 & -0.0457 & 1.0000 & 0.0962\\
  & -0.0423 & 0.0490 & -0.0277 & 0.0962 & 1.0000
\end{pmatrix}
\]

##  Task 2 - Results Presentation(CI of $mu$ and $Beta_mean$)
```{r warning=FALSE, echo=FALSE}
data <- data.frame(
  variable = c("beta_mean_0", "beta_mean_1", "beta_mean_2", "beta_mean_3", "beta_mean_4",
               "mu_0", "mu_1", "mu_2", "mu_3", "mu_4"),
  mean = c(-3.6630742, 0.9025330, -0.0585258, -0.4508275, 0.4722175,
           4.6170057, 0.9035517, -0.0425631, -0.4558512, 0.4706897),
  lower_CI = c(-4.9777068, 0.8896760, -0.2852565, -0.5631008, 0.4325184,
               -4.6014745, 0.8399851, -0.2496227, -0.5891481, 0.3979018),
  upper_CI = c(-1.1445976, 0.9156107, 0.1540752, -0.3268690, 0.5126448,
               22.2632268, 0.9661758, 0.1581210, -0.3209113, 0.5432620)
)


knitr::kable(data, format = "markdown")

```

## Task 3
**Objective:**

Compute posterior summaries and 95% credible intervals of $\gamma$, the fixed effects associated with the covariates in the model. Using the estimated Bayesian model, answer the following questions:\par
(1) Are there seasonal differences in hurricane wind speeds?\par
(2) Is there evidence to support the claim that hurricane wind speeds have been increasing over the years?

## Task 3 - Parameters Convergence
\begin{figure}[H] 
\includegraphics[width=1.0\textwidth]{parameters3.0/mu/parameter_1.png} 
\end{figure}

## Task 4 - Objective
**Objective:**

With the estimated model parameters and covariate values, you can calculate the predicted wind speed for each time point using the model equation. This way, you can track the hurricane and compare the predicted wind speeds with the actual wind speeds recorded during the hurricane. Please evaluate how well the estimated Bayesian model can track individual hurricanes.

## Task 4
**Prediction:**
Using the parameters after burn-in, we can obtain the predicted value for each hurricane.
$$Y_{i}(t+6) =\beta_{0,i}+\beta_{1,i}Y_{i}(t) + \beta_{2,i}\Delta_{i,1}(t)+
\beta_{3,i}\Delta_{i,2}(t) +\beta_{4,i}\Delta_{i,3}(t)  + \mathbf{X}_i{^\top\boldsymbol\gamma}$$ 

**Performance evaluation:**
For each hurricane, we can evaluate the estimated Bayesian model performance by calculating
$$
RMSE = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{n}}
$$
$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

## Task 4
\begin{table}[h]
\centering
\small
\caption{Summary of RMSE and R-squared for selected hurricanes}
\label{table:summary}
\begin{tabular}{llcc}
\hline
\textbf{ID} & \textbf{Year} & \textbf{RMSE} & \textbf{R-squared} \\ \hline
ABBY.1960 & 1960 & 8.8804 & 0.7700 \\
ABBY.1964 & 1964 & 9.6430 & 0.3033 \\
ABBY.1968 & 1968 & 3.5043 & 0.9360 \\
ABLE.1950 & 1950 & 3.6755 & 0.9813 \\
ABLE.1951 & 1951 & 3.4802 & 0.9767 \\
ABLE.1952 & 1952 & 4.5183 & 0.9583 \\
AGNES.1972 & 1972 & 5.2483 & 0.8881 \\
ALBERTO.1982 & 1982 & 8.0473 & 0.7499 \\
ALBERTO.1988 & 1988 & 2.6121 & 0.7420 \\
ALBERTO.1994 & 1994 & 4.3941 & 0.8807 \\
ALBERTO.2000 & 2000 & 3.7896 & 0.9625 \\
ALBERTO.2006 & 2006 & 4.3591 & 0.7882 \\
ALBERTO.2012 & 2012 & 3.2193 & 0.8036 \\
ALEX.1998 & 1998 & 2.9351 & 0.7289 \\
ALEX.2004 & 2004 & 5.4552 & 0.9539 \\ \hline
\end{tabular}
\end{table}

## Task 4
Prediction performance on random chosen example hurricanes.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{predic_plots/b_1_4.png}
  \caption{Time series prediction plot}
\end{figure}

## Task 4
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{predic_plots/a_1_4.png}
  \caption{Prediction vs. observation}
\end{figure}

## Discussions

- Parameters Convergence Problem

## Reference

Reference

## Q&A

- Thanks for listening!